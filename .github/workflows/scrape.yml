name: Scrape Opsera Press Releases

on:
  # Run every week on Monday at 9 AM UTC
  schedule:
    - cron: '0 9 * * 1'

  # Allow manual trigger from GitHub Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Chrome
        uses: browser-actions/setup-chrome@latest

      - name: Install ChromeDriver
        uses: nanasess/setup-chromedriver@v2

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        env:
          GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_CREDENTIALS }}
          SHEET_NAME: ${{ secrets.SHEET_NAME }}
          DISPLAY: ':99'
        run: |
          # Start virtual display for headless Chrome
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 2

          # Run the scraper
          python scraper.py

      - name: Upload scraped data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data
          path: scraped_data.json
          retention-days: 30
